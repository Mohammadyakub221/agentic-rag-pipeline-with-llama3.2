# ğŸ¤– Agentic RAG Pipeline â€” Local Document Intelligence System  

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)  
[![LLM](https://img.shields.io/badge/LLM-Llama%203.2-orange.svg)](https://ollama.com/library/llama3.2)  
[![Framework](https://img.shields.io/badge/Framework-LangChain-green.svg)](https://www.langchain.com/)  
[![VectorDB](https://img.shields.io/badge/VectorDB-Chroma-yellow.svg)](https://www.trychroma.com/)  

An **Agentic Retrieval-Augmented Generation (RAG) System** that intelligently decides whether to retrieve information from local documents or answer directly using the LLMâ€™s built-in knowledge â€” all while running **fully offline and privacy-first**.

---

## ğŸš€ Project Overview  

Traditional RAG pipelines perform vector search for **every query**, even when retrieval is unnecessary. This wastes compute and increases response latency.

This project introduces an **Agentic Router Layer** that analyzes user intent and dynamically routes the query to:

- ğŸ“š **Document Retrieval Pipeline** (ChromaDB + Embeddings)  
- ğŸ§  **Direct LLM Response Path** (No retrieval overhead)  

Result:  
âœ” Faster responses  
âœ” Lower compute cost  
âœ” Smarter pipeline behavior  
âœ” Fully local execution  

---

## âœ¨ Key Features  

- **Agentic Query Routing**  
  Automatically classifies whether the question needs document context or general LLM knowledge.

- **Privacy-First Architecture**  
  Runs entirely on your local machine using Ollama. No cloud APIs. No data leaks.

- **Semantic Document Search**  
  Uses `nomic-embed-text` embeddings to retrieve context based on meaning, not keywords.

- **Persistent Vector Storage**  
  ChromaDB stores document embeddings locally for fast reuse.

- **Modular Pipeline Design**  
  Easily extendable for new agents, memory layers, or tools.

---

## ğŸ§  System Architecture  

```mermaid
graph TD
    A[User Query] --> B{Agentic Router}
    
    B -- Needs Document Context --> C[Chroma Vector Database]
    B -- General Knowledge --> D[LLM: Llama 3.2]
    
    C --> E[Relevant Chunks Retrieved]
    E --> D
    
    D --> F[Final Response]
```

---

## ğŸ—ï¸ Tech Stack  

| Component | Technology |
-----------|------------
LLM Engine | Ollama + Llama 3.2  
Embeddings | nomic-embed-text  
Framework | LangChain  
Vector Store | ChromaDB  
Language | Python 3.10+  

---

## ğŸ“‚ Project Structure  

```
agentic-rag-pipeline/
â”‚
â”œâ”€â”€ data/                  # PDF documents
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py             # Entry point
â”‚   â”œâ”€â”€ router.py           # Agentic routing logic
â”‚   â”œâ”€â”€ retriever.py        # Vector retrieval pipeline
â”‚   â”œâ”€â”€ llm_handler.py      # LLM interaction
â”‚
â”œâ”€â”€ chroma_db/              # Persistent vector store
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation Guide  

### 1ï¸âƒ£ Install Ollama  

Download and install Ollama:

https://ollama.com  

Pull required models:

```bash
ollama pull llama3.2
ollama pull nomic-embed-text
```

---

### 2ï¸âƒ£ Clone Repository  

```bash
git clone https://github.com/Mohammadyakub221/agentic-rag-pipeline-with-llama3.2.git
cd agentic-rag-pipeline-with-llama3.2
```

---

### 3ï¸âƒ£ Create Virtual Environment  

```bash
python -m venv venv
```

Activate environment:

Windows:
```bash
.\venv\Scripts\activate
```

Linux / Mac:
```bash
source venv/bin/activate
```

---

### 4ï¸âƒ£ Install Dependencies  

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage  

### Step 1 â€” Add Documents  

Place your PDF files inside:

```
/data
```

---

### Step 2 â€” Run Pipeline  

```bash
python src/main.py
```

---

### Step 3 â€” Ask Questions  

Examples:

Triggers Retrieval:
```
"What does the financial report PDF say about revenue growth?"
```

Direct LLM Answer:
```
"What is Retrieval Augmented Generation?"
```

---

## ğŸ§ª Example Use Cases  

- Internal company document assistant  
- Research paper Q&A system  
- Offline knowledge base chatbot  
- Secure enterprise RAG pipeline  
- Hackathon-ready AI assistant  

---

## ğŸ›¡ï¸ Privacy & Security  

- No cloud APIs used  
- No internet dependency after setup  
- All data remains on-device  
- Fully offline execution  

---

## ğŸ”® Future Enhancements  

- Multi-agent orchestration  
- Streaming token responses  
- Web UI (Streamlit / React)  
- Multi-document ranking  
- Chat memory integration  
- Tool calling support  

---

## ğŸ“œ License  

MIT License â€” Free to use, modify, and distribute.

---

## ğŸ‘¨â€ğŸ’» Author  

**Mohammad Yakub**  
AI & Data Science Engineer  
GitHub: https://github.com/Mohammadyakub221  

---
